{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook solves the 2site hexagonal lattice hubbard model using NNgHMC method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hamiltonian required for 2-site Hubbard model is :\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    H & = \\frac{\\vec{p}^2}{2} + \\frac{\\vec{\\phi}^2}{2\\tilde{U}} - \\log\\det \\left[M[\\phi,\\tilde{\\kappa},\\tilde{\\mu}]M[-\\phi,-\\tilde{\\kappa},-\\tilde{\\mu}]\\right] \\\\ & \\equiv  \\frac{\\vec{p}^2}{2} + S_{eff}[\\phi], \n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "where the effective action $S_{eff}$ comes from the Hubbard-Stratonovich transformation: \n",
    "\\begin{equation}\n",
    "    S_{eff}[\\phi] = \\frac{\\vec{\\phi}^2}{2\\tilde{U}} - \\log\\det \\left[M[\\phi,\\tilde{\\kappa},\\tilde{\\mu}]M[-\\phi,-\\tilde{\\kappa},-\\tilde{\\mu}]\\right].\n",
    "\\end{equation}\n",
    "and the gradient of $S_{eff}$ is what the NN approximates:\n",
    "    \n",
    "\\begin{equation}\n",
    "\\partial_{\\phi_{xt}}S_{eff}[\\phi]=\\frac{\\phi_{xt}}{U\\delta}-\\partial_{\\phi_{xt}}\\log\\det M[\\phi]M[-\\phi] =\\frac{\\phi_{xt}}{U\\delta}-2\\operatorname{Re} \\operatorname{tr}\\left(M^{-1}[\\phi]\\partial_{\\phi_{xt}}M[\\phi]\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Here the parameters of the Hubbard model, the potential energy $U$, the chemical potential $\\mu$, and the hopping matrix $\\kappa$ are scaled to the discretized time $\\delta$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta =\\frac{\\beta}{N_t}, \\quad \\tilde{\\kappa}=\\delta\\kappa, \\quad \\tilde{U}=\\delta U, \\quad \\tilde{\\mu} = \\delta\\mu.\n",
    "\\end{equation}\n",
    "\n",
    "The elements of the fermion matrix $M[\\phi,\\tilde{\\kappa},\\tilde{\\mu}]$ can be calculated by *exponential discretization*:\n",
    "    \n",
    "\\begin{equation}\n",
    "    M[\\phi,\\tilde{\\kappa},\\tilde{\\mu}]_{x't';xt} =\\delta_{x',x}\\delta _{t',t}-B_{t'}\\left[e^{h-\\tilde{\\mu}}\\right]_{x',x}e^{i\\phi_{x,t'-1}}\\delta_{t',t+1},\n",
    "\\end{equation}\n",
    "                                                                                                                            \n",
    "with the hopping matrix $h_{x',x}=\\tilde{\\kappa}\\delta_{\\langle x',x\\rangle}$, and the anti-periodic boundary condition:\n",
    "                                                                                                                            \n",
    "\\begin{equation}\n",
    "    B_{t} =\\begin{cases}\n",
    "    +1, & 0<t<N_t \\\\\n",
    "    -1, & t =0.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the modules \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "import scipy.stats as stats\n",
    "from numba import jit\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions required\n",
    "\n",
    "To get the force equations, we need to calculate the following:\n",
    "​\n",
    "\\begin{equation}\n",
    "\\partial_{\\phi_{xt}}\\log\\det M[\\phi]M[-\\phi] = \\operatorname{tr}\\left(M^{-1}[\\phi]\\partial_{\\phi_{xt}}M[\\phi]\\right)+ \\operatorname{tr}\\left(M^{-1}[-\\phi]\\partial_{\\phi_{xt}}M[-\\phi]\\right)\n",
    "\\end{equation}\n",
    "​\n",
    "For this particular problem, since it is bi-partite, half-filling, and we are working in the particle/hole basis, the above equation simplifies to\n",
    "​\n",
    "\\begin{equation}\n",
    "\\partial_{\\phi_{xt}}\\log\\det M[\\phi]M[-\\phi] =2\\operatorname{Re} \\operatorname{tr}\\left(M^{-1}[\\phi]\\partial_{\\phi_{xt}}M[\\phi]\\right)\n",
    "\\end{equation}\n",
    "​\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcExpK(delta):\n",
    "    \"\"\"Function that calculates exponential of hopping matrix\"\"\"\n",
    "    return [[np.cosh(delta),np.sinh(delta)],[np.sinh(delta),np.cosh(delta)]]\n",
    "\n",
    "def Mphi(phi, expk, nt):\n",
    "    \"\"\"\n",
    "    Function that updates the Fermion matrix\n",
    "    \n",
    "    INPUT:\n",
    "            phi - phi field (array)\n",
    "            k - hopping parameter (array)\n",
    "            nt - number of time steps\n",
    "                        \n",
    "    OUTPUT:\n",
    "            M - fermion matrix (update M) \n",
    "            \n",
    "            This uses space index fastest (like Isle)\n",
    "    \"\"\"\n",
    "    n = len(phi)\n",
    "    nx = int(n/nt)\n",
    "    if n == M.shape[0]: # this tests if the phi array has the right dimensions\n",
    "        for t in range(nt-1): # loop over time bloks\n",
    "            for x in range(nx): # loop over cords and kappa matrix\n",
    "                M[t*nx+x][t*nx+x] = 1.0 + 0j # diagonal term\n",
    "                for y in range(nx): # run over the kappa matrix\n",
    "                    M[(t+1)*nx+x][t*nx+y] = -expk[x][y]*np.exp(1j*phi[t*nx+y]) # off-diagonal\n",
    "        for x in range(nx): \n",
    "            M[(nt-1)*nx+x][(nt-1)*nx+x] = 1.0+0j # diagonal term\n",
    "            for y in range(nx): # anti-periodic boundary condition\n",
    "                M[x][(nt-1)*nx+y] = expk[x][y]*np.exp(1j*phi[(nt-1)*nx+y])\n",
    "        return 0\n",
    "    else:\n",
    "        print('# Error! phi and M have inconsistent dimensions!')\n",
    "        return -1\n",
    "    \n",
    "def calcLogDetMM(phi, expk, nt):\n",
    "    \"\"\"\n",
    "    Function that calculates the determinant of Fermion matrices\n",
    "    \n",
    "    INPUT:\n",
    "            phi - phi field (array)\n",
    "            expk - exponential of hopping parameter (array)\n",
    "            Nt - number of time steps\n",
    "                        \n",
    "    OUTPUT:\n",
    "            detMM - determinant of the matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    Mphi(phi, expk, nt) # update M with +1 phi\n",
    "    detMM = np.log(np.linalg.det(M)) # calc detM with +1 phi\n",
    "    Mphi(-np.array(phi), expk, nt) # update M with -1 phi\n",
    "    detMM += np.log(np.linalg.det(M)) # calc detM with -1 phi\n",
    "    return detMM\n",
    "\n",
    "def calcTrMM(phi, expk, nt, sign):\n",
    "    \"\"\"\n",
    "    Function that calculates the trace of a Fermion matrix using exponential discretization\n",
    "    \n",
    "    INPUT:\n",
    "            phi - phi field (array)\n",
    "            expk - exponential of hopping term (array)\n",
    "            nt - number of time steps\n",
    "            sign - sign of phi (+1 / -1)\n",
    "                        \n",
    "    OUTPUT:\n",
    "            TrMM - trace of the matrix\n",
    "    \"\"\"\n",
    "    TrMM = [] # trace container\n",
    "    n = len(phi)\n",
    "    nx = int(n/nt)\n",
    "    Mphi(phi, expk, nt) # update M\n",
    "    invM = np.linalg.inv(M)  # only need to invert once!\n",
    "    for t in range(nt-1): # loop over time blocks\n",
    "        for x in range(nx): # loop over sites  (space is fastest)\n",
    "            temp = 0 + 0j\n",
    "            for y in range(nx):\n",
    "                temp += invM[t*nx+x][(t+1)*nx+y]*expk[y][x]\n",
    "            TrMM.append(temp*(-sign*1j*np.exp(1j*phi[t*nx+x])))\n",
    "    for x in range(nx): # anti-periodic boundry conditions\n",
    "        temp = 0 + 0j\n",
    "        for y in range(nx):\n",
    "            temp += invM[(nt-1)*nx+x][y]*expk[y][x]\n",
    "        TrMM.append(temp*sign*1j*np.exp(1j*phi[(nt-1)*nx+x]))\n",
    "\n",
    "    return np.array(TrMM)\n",
    "\n",
    "def artH(p, phi, expk, nt, U):\n",
    "    \"\"\"\n",
    "    Function that calculates the artificial Hamiltonian of the Hubbard model\n",
    "    \n",
    "    INPUT:\n",
    "            p - conjugate momentum (array)\n",
    "            phi - phi field (array)            \n",
    "            expk - exponential of hopping array (array)\n",
    "            Nt - number of timesteps\n",
    "            U - onsite coupling (reduced quantity = U * delta)\n",
    "            \n",
    "            (optional)\n",
    "            beta = inverse temperature\n",
    "                        \n",
    "    OUTPUT:\n",
    "            H - artificial Hamiltonian\n",
    "    \"\"\"\n",
    "        \n",
    "    H = .5*(np.array(p)@np.array(p)+np.array(phi)@np.array(phi)/U) \n",
    "    \n",
    "    H -= np.real(calcLogDetMM(phi, expk, nt))\n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "def gradS(phi,U,Nt):\n",
    "    \"\"\"function that calculates gradient\"\"\"\n",
    "    return phi/U - 2*np.real(calcTrMM(phi,expk,Nt,1))\n",
    "\n",
    "def ribbit(p, phi, expk, nt, U, Nmd,eps, surrogate, trajLength = 1.):\n",
    "    \"\"\"\n",
    "    Molecular dynamics integrator (Leap frog algorithm)\n",
    "    \n",
    "    INPUT:\n",
    "            p - conjugate momentum (array)\n",
    "            phi - phi field (array)\n",
    "            expk - exponential of hopping parameter (array)\n",
    "            nt - number of timesteps\n",
    "            Nmd - number of trajectory pieces\n",
    "            U - onsite coupling\n",
    "            eps - integration step\n",
    "            surrogate - NN trained gradient\n",
    "            (optional)\n",
    "            trajLength - length of trajectory\n",
    "            \n",
    "            \n",
    "    OUTPUT:\n",
    "            (p, phi) - after integration\n",
    "    \"\"\"    \n",
    "\n",
    " \n",
    "    p = np.array([p[i] for i in range(len(p))])\n",
    "    phi = np.array([phi[i] for i in range(len(phi))])\n",
    "    \n",
    "\n",
    "    \n",
    "    phi += 0.5*eps*p # first half step\n",
    "    \n",
    "    # steps of integration\n",
    "    #L = int(np.random.uniform(0, 1)* Nmd)\n",
    "    for _ in range(Nmd-1):\n",
    "        if surrogate is None:\n",
    "            p -= eps*gradS(phi,Udelta,nt)\n",
    "        else:            \n",
    "            p -= eps*surrogate.gradient(phi)          #<====== replaced with NN\n",
    "        phi += eps*p\n",
    "\n",
    "    # last half step\n",
    "    if surrogate is None:\n",
    "        p -= eps*gradS(phi,Udelta,nt)      \n",
    "    else:\n",
    "        p -= eps*surrogate.gradient(phi)             #<====== replaced with NN\n",
    "    phi += 0.5*eps*p\n",
    "    \n",
    "    return p, phi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def HMC(Nmd,eps,surrogate):\n",
    "    \"\"\" \n",
    "    Function performs the HMC\n",
    "    \n",
    "    INPUT:\n",
    "    Nmd : Number of molecular dynamic steps\n",
    "    eps : integration step\n",
    "    surrogate : None for Normal else NNgHMC\n",
    "    \n",
    "    \n",
    "    OUTPUT:\n",
    "    ensemble : final phi's\n",
    "    prob : acceptance probability\n",
    "    \n",
    "    \"\"\"\n",
    "    prob = [] # stores probability\n",
    "    ensemble = []  # store the individual configurations here\n",
    "\n",
    "    # sample phi from normal distribution with sigma = sqrt(u)\n",
    "    phi = np.array([np.random.normal(0,usqrt) for i in range(Nt*Nx)])\n",
    "\n",
    "    for traj in tqdm(range(nTrajs)):\n",
    "        # sample momentum from normal distribution w/ sigma = 1\n",
    "        initP = np.array([np.random.normal(0,1) for i in range(Nt*Nx)])\n",
    "\n",
    "        initPhi = phi\n",
    "\n",
    "        initH = artH(initP, initPhi, expk, Nt, Udelta) # initial Hamiltonian\n",
    "\n",
    "        finP, finPhi = ribbit(initP, initPhi, expk, Nt, Udelta, Nmd,eps,surrogate)\n",
    "\n",
    "        finH = artH(finP, finPhi, expk, Nt, Udelta) # final hamiltonian\n",
    "\n",
    "        # accept/reject step\n",
    "        if np.random.uniform(0,1) <= np.exp(-np.real(finH-initH)): # accept\n",
    "            phi = finPhi\n",
    "            ensemble.append(phi)\n",
    "            prob.append(1.)\n",
    "        else: # reject\n",
    "            ensemble.append(phi)\n",
    "            prob.append(0.)\n",
    "\n",
    "    prob = np.array(prob)\n",
    "\n",
    "    return ensemble, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants for the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize constants for the functions\n",
    "U=2. # spin coupling\n",
    "beta=4. # inverse temperature\n",
    "Nt=16 # number of time steps\n",
    "Nx = 2 # number of sites\n",
    "delta = beta/Nt # discretised time\n",
    "Udelta = delta*U  #reduced U\n",
    "usqrt = np.sqrt(Udelta) \n",
    "M = np.identity(Nt*Nx) + 0j # (Nt*Ni) x (Nt*Ni) identity matrix\n",
    "expk = calcExpK(delta) # calc exp(kappa)\n",
    "#Nmd = 10# 3-5 for best acceptance >= 70%\n",
    "nTrajs = 10000 # number of auxiliary fields in the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_HMC,actualHMC_prob = HMC(Nmd=5,eps=1.0/5.0,surrogate = None)\n",
    "print(\"actual HMC prob\",actualHMC_prob.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data from radnom sampling of normal distribution\n",
    "num_samples = 10000\n",
    "training_gaus = np.zeros((num_samples,Nt*Nx))\n",
    "gradient_gaus = np.zeros((num_samples,Nt*Nx))\n",
    "\n",
    "data_actual = np.array(actual_HMC)\n",
    "gradient_actual = np.zeros((len(actual_HMC),Nt*Nx))\n",
    "\n",
    "#stores data from Actual HMCdatata\n",
    "xx = np.zeros((len(actual_HMC)+ num_samples,Nt*Nx))\n",
    "yy= np.zeros((len(actual_HMC)+ num_samples,Nt*Nx))\n",
    "\n",
    "for i in range(num_samples): \n",
    "    training_gaus[i,:] = np.random.normal(0,usqrt,Nt*Nx)\n",
    "    gradient_gaus[i,:] = gradS(training_gaus[i,:],U*delta,Nt)\n",
    "    gradient_actual[i,:] = gradS(data_actual[i,:],U*delta,Nt)\n",
    "    \n",
    "\n",
    "\n",
    "xx[:num_samples,:] = training_gaus[:num_samples,:]\n",
    "xx[num_samples:,:] = data_actual[:,:]\n",
    "\n",
    "yy[:num_samples,:] = gradient_gaus[:num_samples,:]\n",
    "yy[num_samples:,:] = gradient_actual[:,:]\n",
    "\n",
    "\n",
    "plt.hist(data_actual.flatten(),density=True,bins=30,label='training_gaus')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gradient_actual.flatten(),density=True,bins='auto',label='HMC')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation fot training NN in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x = xx\n",
    "# y = yy\n",
    "\n",
    "# scaled_data = preprocessing.StandardScaler().fit(x) #scaling the data to zero mean and unit variance\n",
    "# x = preprocessing.scale(x)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.20)\n",
    "\n",
    "# trial_data = np.zeros((num_samples,Nt*Nx)) # data for testing the NN\n",
    "# for i in range(num_samples): \n",
    "#     trial_data[i,:] = np.random.normal(0,usqrt,Nt*Nx)\n",
    "    \n",
    "# trial_scaled = preprocessing.StandardScaler().fit(trial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Train the NN to approximate gradient with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the NN to approximate gradient\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# from models import exper, build\n",
    "\n",
    "# epoch = 80\n",
    "# batch = 150\n",
    "# model = exper(32, [64] , 32)\n",
    "# #model = build(32,[64],[32])\n",
    "\n",
    "# r = model.fit(x,y,epochs=epoch, batch_size=batch,verbose=2)\n",
    "\n",
    "# #r = model.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=epoch, batch_size=batch,verbose=2)\n",
    "\n",
    "\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print('Accuracy: %.2f'% (accuracy*100), 'Loss: %.2f' % (loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Using the NN inside HMC to sample points\n",
    "# from surrogate import NeuralGrad\n",
    "# grad_hat = NeuralGrad(model,trial_scaled)\n",
    "# NNg ,NNg_prob = HMC(surrogate = grad_hat, training = False)\n",
    "# print(\"actualHMC_prob=\",actualHMC_prob.mean())\n",
    "# print(\"NNg_prob=\",NNg_prob.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the NN to approximate gradient with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import chain\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class NNg(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: int for input dimension\n",
    "        hidden_dim: list of int for multiple hidden layers\n",
    "        output_dim: list of int for multiple output layers\n",
    "        \"\"\"\n",
    "\n",
    "        # calling constructor of parent class\n",
    "        super().__init__()\n",
    "\n",
    "        # defining the inputs to the first hidden layer\n",
    "        self.hid1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        nn.init.normal_(self.hid1.weight, mean=0.0, std=0.01)\n",
    "        nn.init.zeros_(self.hid1.bias)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "#         self.act1 = nn.SiLU()\n",
    "\n",
    "        # defining the inputs to the output layer\n",
    "        self.hid2 = nn.Linear(hidden_dim[0], output_dim)\n",
    "        nn.init.xavier_uniform_(self.hid2.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # input and act for layer 1\n",
    "        X = self.hid1(X)\n",
    "        X = self.act1(X)\n",
    "\n",
    "        # input and act for layer 2\n",
    "        X = self.hid2(X)\n",
    "        return X\n",
    "\n",
    "epochs = 180\n",
    "batchsize = 150\n",
    "#to access a small section of the training data using the array indexing\n",
    "inputs,targets = torch.from_numpy(xx).float(),torch.from_numpy(yy).float()\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "# split the data into batches\n",
    "train_dl = DataLoader(train_ds, batchsize,shuffle=False)\n",
    "test_dl = DataLoader(train_ds, batchsize, shuffle= False)\n",
    "model_torch = NNg(Nt*Nx,[2*Nt*Nx],Nt*Nx)\n",
    "optimizer = torch.optim.Adam(model_torch.parameters(),weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(model_torch.parameters(),lr=0.01,weight_decay=1e-5)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "train_losses =[]\n",
    "\n",
    "# iterate through all the epoch\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    running_loss=0\n",
    "    # go through all the batches generated by dataloader\n",
    "    for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model_torch(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets.type(torch.FloatTensor))\n",
    "            loss_plot.append(loss)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    train_loss=running_loss/len(train_dl)\n",
    "    train_losses.append(train_loss)\n",
    "    #Print the progress\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch +1, epochs, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss plot\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(train_losses,label=\"training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the NN inside HMC to sample points\n",
    "from surrogate import NeuralGrad_pytorch\n",
    "grad_hat = NeuralGrad_pytorch(model_torch)\n",
    "phi_NNg ,p_NNg_prob = HMC(Nmd=5,eps=1/5.,surrogate = grad_hat)\n",
    "print(\"actualHMC_prob=\",actualHMC_prob.mean())\n",
    "print(\"NNg_prob=\",p_NNg_prob.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exact co-relators data \n",
    "exactData = open(\"exactFiles/U4B6.dat\").readlines()\n",
    "exT = []\n",
    "exBonding = []\n",
    "exAntiBonding = []\n",
    "exAA = []\n",
    "exAB = []\n",
    "exBA = []\n",
    "exBB = []\n",
    "for i in range(len(exactData)):\n",
    "    split = exactData[i].split()\n",
    "    exT.append(float(split[0]))   # tau\n",
    "    exAntiBonding.append(float(split[1]))  # anti-bonding\n",
    "    exBonding.append(float(split[2]))      # bonding\n",
    "    exAA.append(float(split[3]))  # cAA\n",
    "    exAB.append(float(split[4]))  # cAB\n",
    "    exBA.append(float(split[5]))  # cBA\n",
    "    exBB.append(float(split[6]))  # cBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating co-relators \n",
    "\n",
    "def correlators(sample):\n",
    "\n",
    "    corrUp_b = [ [] for t in range(Nt)] # <===== each correlator has Nt elements\n",
    "    corrUp_ab = [ [] for t in range(Nt)]\n",
    "    Cxx = [ [] for t in range(Nt)]\n",
    "    Cxy = [ [] for t in range(Nt)]\n",
    "    Cyx = [ [] for t in range(Nt)]\n",
    "    Cyy = [ [] for t in range(Nt)]\n",
    "\n",
    "    nTherm = 200\n",
    "\n",
    "    ### calculate the bonding/anti-bonding correlator\n",
    "    for i in range(nTherm,nTrajs):\n",
    "        # arrow up correlator\n",
    "        phi = sample[i]\n",
    "        Mphi(phi,expk,Nt)\n",
    "        invMUp = np.linalg.inv(M)\n",
    "\n",
    "        # we now construct the correlators\n",
    "        # bonding correlator is .5*(Cxx+Cxy+Cyx+Cyy)\n",
    "        # antibonding is .5*(Cxx-Cxy-Cyx+Cyy)\n",
    "        for t in range(Nt):\n",
    "            corrUp_b[t].append(np.real(.5*(invMUp[t*Nx+0][0*Nx+0]+invMUp[t*Nx+0][0*Nx+1]+\n",
    "                                   invMUp[t*Nx+1][0*Nx+0]+invMUp[t*Nx+1][0*Nx+1])))\n",
    "            corrUp_ab[t].append(np.real(.5*(invMUp[t*Nx+0][0*Nx+0]-invMUp[t*Nx+0][0*Nx+1]-\n",
    "                                   invMUp[t*Nx+1][0*Nx+0]+invMUp[t*Nx+1][0*Nx+1])))\n",
    "            Cxx[t].append(np.real(invMUp[t*Nx+0][0*Nx+0]))\n",
    "            Cxy[t].append(np.real(invMUp[t*Nx+0][0*Nx+1]))\n",
    "            Cyx[t].append(np.real(invMUp[t*Nx+1][0*Nx+0]))\n",
    "            Cyy[t].append(np.real(invMUp[t*Nx+1][0*Nx+1]))\n",
    "\n",
    "    # here I set up arrays that store the averages\n",
    "    corrBond = []\n",
    "    corrAntiBond = []\n",
    "\n",
    "    # ok, I'm mixing up naming conventions.  Here 'A' = 'x' and 'B' = 'y'\n",
    "    cAA = []\n",
    "    cAB = []\n",
    "    cBA = []\n",
    "    cBB = []\n",
    "\n",
    "    # now I calculate the averages for each timeslice\n",
    "    for t in range(Nt):\n",
    "        corrBond.append(np.mean(corrUp_b[t]))\n",
    "        corrAntiBond.append(np.mean(corrUp_ab[t]))\n",
    "        cAA.append(np.mean(Cxx[t]))\n",
    "        cAB.append(np.mean(Cxy[t]))\n",
    "        cBA.append(np.mean(Cyx[t]))\n",
    "        cBB.append(np.mean(Cyy[t]))\n",
    "\n",
    "    tau = np.linspace(0,beta-beta/Nt,Nt) # <===== this is the correct distancing for tau\n",
    "    return corrBond,corrAntiBond,cAA,cAB,cBA,cBB,tau,corrUp_b,corrUp_ab,Cxx,Cxy,Cyx,Cyy\n",
    "\n",
    "a_corrBond,a_corrAntiBond,a_cAA,a_cAB,a_cBA,a_cBB,a_tau,a_corrUp_b,a_corrUp_ab,a_Cxx,a_Cxy,a_Cyx,a_Cyy = correlators(actual_HMC)\n",
    "corrBond,corrAntiBond,cAA,cAB,cBA,cBB,tau,corrUp_b,corrUp_ab,Cxx,Cxy,Cyx,Cyy = correlators(phi_NNg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning and Bootstraping\n",
    "def binAndBoot(corr,nBN,nBS,check):\n",
    "    # corr is the array of of correlators \n",
    "    # nBN is the number or elements to bin\n",
    "    # nBS is the number of bootstrap samples\n",
    "    if check == 1:\n",
    "        # making the correlators in correct form [samples]x[Nt]\n",
    "        correlators = []\n",
    "        for i in range(len(corr[0])): # range(samples)\n",
    "            correlators.append([corr[t][i] for t in range(len(corr))]) #range(Nt)\n",
    "    else:\n",
    "        correlators = corr\n",
    "        \n",
    "    corrBN = [] #shape [(samples)/nBN x Nt]\n",
    "    for i in range(int(len(correlators)/nBN)): #range(samples/nBN)\n",
    "        corrBN.append(np.mean(correlators[i*nBN:(i+1)*nBN],axis=0))\n",
    "        \n",
    "    n = len(corrBN)  # length of ensemble (samples/nBN)\n",
    "    bsMeans = []\n",
    "    \n",
    "    # here are my bootstrap indices\n",
    "    bsSamples = [ np.random.randint(n,size=n) for bs in range(nBS)]  #shape [nBS x (samples/nBN) ]\n",
    "    for sample in bsSamples: #shape [(samples/nBN) ]\n",
    "        bsMeans.append(np.mean([corrBN[index] for index in sample],axis=0))   # mean along rows( [(samples/nBN) x Nt]) \n",
    "    # bsMeans.append([1 x Nt]), shape [nBS x 16]\n",
    "    # returns array of shape[1 x Nt]\n",
    "    return np.std(bsMeans,axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting correlators\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(15,15))\n",
    "\n",
    "\n",
    "\n",
    "axx = ax[0][0]\n",
    "#axx.plot(a_tau,a_corrBond,'ro',label= r'$HMCBonding$')\n",
    "axx.errorbar(a_tau,a_corrBond,yerr=binAndBoot(a_corrUp_b,100,100,1),marker='o',label='HMCBonding')\n",
    "\n",
    "#axx.plot(a_tau,a_corrAntiBond,'bo', label = '$HMCAntibonding$')\n",
    "axx.errorbar(a_tau,a_corrAntiBond,yerr=binAndBoot(a_corrUp_ab,100,100,1),marker='o',label='HMCAntiBonding')\n",
    "\n",
    "axx.plot(exT,exBonding,'k--',label='exact')\n",
    "axx.plot(exT,exAntiBonding,'k--')\n",
    "\n",
    "axx.grid()\n",
    "axx.set_xlabel(r'$t$',fontsize=18)\n",
    "axx.set_yscale('log')\n",
    "axx.legend(loc='best')\n",
    "\n",
    "axx = ax[0][1]\n",
    "#axx.plot(tau,corrBond,'go',label= r'$NNgBonding$')\n",
    "axx.errorbar(tau,corrBond,yerr=binAndBoot(corrUp_b,100,100,1),marker='o',label='NNgHMCBonding')\n",
    "\n",
    "#axx.plot(tau,corrAntiBond,'yo', label = '$NNgAntibonding$')\n",
    "axx.errorbar(tau,corrAntiBond,yerr=binAndBoot(corrUp_ab,100,100,1),marker='o',label='NNgHMCAntiBonding')\n",
    "\n",
    "axx.plot(exT,exBonding,'k--',label='exact')\n",
    "axx.plot(exT,exAntiBonding,'k--')\n",
    "\n",
    "axx.grid()\n",
    "axx.set_xlabel(r'$t$',fontsize=18)\n",
    "axx.set_yscale('log')\n",
    "axx.legend(loc='best')\n",
    "\n",
    "axx = ax[1][0]\n",
    "axx.errorbar(a_tau,a_corrBond,yerr=binAndBoot(a_corrUp_b,100,100,1),marker='o',label='HMCBonding')\n",
    "axx.errorbar(tau,corrAntiBond,yerr=binAndBoot(corrUp_ab,100,100,1),marker='o',label='NNgHMCAntiBonding')\n",
    "\n",
    "axx.errorbar(a_tau,a_corrAntiBond,yerr=binAndBoot(a_corrUp_ab,100,100,1),marker='o',label='HMCAntiBonding')\n",
    "axx.errorbar(tau,corrBond,yerr=binAndBoot(corrUp_b,100,100,1),marker='o',label='NNgHMCBonding')\n",
    "\n",
    "axx.plot(exT,exBonding,'k--',label='exact')\n",
    "axx.plot(exT,exAntiBonding,'k--')\n",
    "\n",
    "axx.grid()\n",
    "axx.set_xlabel(r'$t$',fontsize=18)\n",
    "axx.set_yscale('log')\n",
    "axx.legend(loc='best')\n",
    "\n",
    "axx = ax[1][1]\n",
    "axx.errorbar(exT[::50],exBonding[::50],yerr=binAndBoot(exBonding[::50],100,100,0),marker='o',label='exactBonding')\n",
    "axx.errorbar(exT[::50],exAntiBonding[::50],yerr=binAndBoot(exAntiBonding[::50],100,100,0),marker='o',label='exactAntibonding')\n",
    "axx.set_xlabel(r'$t$',fontsize=18)\n",
    "axx.set_yscale('log')\n",
    "axx.grid()\n",
    "axx.legend(loc='best')\n",
    "\n",
    "plt.suptitle('U={}, beta={}, Nt={}\\nsamples={},prob_HMC={},prob_NNgHMC={}'\n",
    "            .format(U,beta,Nt,nTrajs,actualHMC_prob.mean(),p_NNg_prob.mean()),fontsize=20)\n",
    "\n",
    "\n",
    "#axx.plot(exT[::50],exBonding[::50],'ro',label='exactBonding')\n",
    "#axx.plot(exT[::50],exAntiBonding[::50],'bo',label='exactAntibonding')\n",
    "#axx.plot(np.array(a_corrBond) - np.array(corrBond),'ro',label= r'$diff_Bonding$')\n",
    "#axx.plot(np.array(a_corrAntiBond) - np.array(corrAntiBond),'bo', label = '$diff_Antibonding$')\n",
    "\n",
    "# plt.savefig(\"testM_U{}_B{}_10000.pdf\".format(U,beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2,figsize=(15,10),sharex=True)\n",
    "\n",
    "aax = ax[0][0]\n",
    "aax.plot(exT,exAA,'k--',label='exact')\n",
    "\n",
    "#aax.plot(tau,cAA,'b.',label='NNgMC')\n",
    "aax.errorbar(tau,cAA,yerr=binAndBoot(Cxx,100,400,1),marker='o',label='NNgHMC')\n",
    "\n",
    "#aax.plot(a_tau,a_cAA,'r.',label='HMC')\n",
    "aax.errorbar(a_tau,a_cAA,yerr=binAndBoot(a_Cxx,100,400,1),marker='o',label='HMC')\n",
    "\n",
    "aax.set_yscale('log')\n",
    "aax.legend(loc='best')\n",
    "aax.set_ylabel(r'$C_{AA}(\\tau)$')\n",
    "aax.grid()\n",
    "\n",
    "aax = ax[0][1]\n",
    "aax.plot(exT,exAB,'k--',label='exact')\n",
    "\n",
    "#aax.plot(tau,cAB,'b.',label='NNgHMC')\n",
    "aax.errorbar(tau,cAB,yerr=binAndBoot(Cxy,100,400,1),marker='o',label='NNgHMC')\n",
    "\n",
    "#aax.plot(a_tau,a_cAB,'r.',label='HMC')\n",
    "aax.errorbar(a_tau,a_cAB,yerr=binAndBoot(a_Cxy,100,400,1),marker='o',label='HMC')\n",
    "\n",
    "\n",
    "aax.set_ylabel(r'$C_{AB}(\\tau)$')\n",
    "aax.legend(loc='best')\n",
    "aax.grid()\n",
    "\n",
    "aax = ax[1][0]\n",
    "aax.plot(exT,exBA,'k--',label='exact')\n",
    "\n",
    "#aax.plot(tau,cBA,'b.',label='NNgHMC')\n",
    "aax.errorbar(tau,cBA,yerr=binAndBoot(Cyx,100,400,1),marker='o',label='NNgHMC')\n",
    "\n",
    "#aax.plot(a_tau,a_cBA,'r.',label='HMC')\n",
    "aax.errorbar(a_tau,a_cBA,yerr=binAndBoot(a_Cyx,100,400,1),marker='o',label='HMC')\n",
    "\n",
    "\n",
    "aax.grid()\n",
    "aax.set_xlabel(r'$\\tau$')\n",
    "aax.set_ylabel(r'$C_{BA}(\\tau)$')\n",
    "aax.legend(loc='best')\n",
    "\n",
    "aax = ax[1][1]\n",
    "aax.plot(exT,exBB,'k--',label='exact')\n",
    "\n",
    "#aax.plot(tau,cBB,'b.',label='NNgHMC')\n",
    "aax.errorbar(tau,cBB,yerr=binAndBoot(Cyy,100,400,1),marker='o',label='NNgHMC')\n",
    "\n",
    "#aax.plot(a_tau,a_cBB,'r.',label='HMC')\n",
    "aax.errorbar(a_tau,a_cBB,yerr=binAndBoot(a_Cyy,100,400,1),marker='o',label='HMC')\n",
    "\n",
    "aax.grid()\n",
    "aax.set_yscale('log')\n",
    "aax.set_xlabel(r'$\\tau$')\n",
    "aax.set_ylabel(r'$C_{BB}(\\tau)$')\n",
    "aax.legend(loc='best')\n",
    "\n",
    "plt.suptitle('U={}, beta={}, Nt={}\\nsamples={},prob_HMC={},prob_NNgHMC={}'\n",
    "            .format(U,beta,Nt,nTrajs,actualHMC_prob.mean(),p_NNg_prob.mean()),fontsize=20)\n",
    "\n",
    "# plt.savefig(\"testM_correlator_logerror_U{}_B{}_10000.pdf\".format(U,beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
