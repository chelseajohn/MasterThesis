{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import the modules \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "import scipy.stats as stats\n",
    "from numba import jit\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import callbacks, optimizers\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tqdm.keras import TqdmCallback\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers\n",
    "from keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 22\n",
    "\n",
    "plt.rc('font', size=12)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=12)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=10)    # fontsize of the x and y labels\n",
    "# plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "# plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Swish Activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import sigmoid\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data with fermion matrix calculated using exponential discretization\n",
    "exp_input_file = \"Expinputs_2sites_U2.0B4.0Nt16.npy\"\n",
    "exp_target_file = \"ExpHex2sitesNt16/targets_2sites_U2.0B4.0Nt16.npy\" \n",
    "\n",
    "#Training data with fermion matrix calculated using diagonal discretization\n",
    "dia_input_file = \"inputs_2sites_U2.0B4.0Nt16.npy\"\n",
    "dia_target_file = \"targets_2sites_U2.0B4.0Nt16.npy\" #diag\n",
    "\n",
    "dia_x = np.load(dia_input_file)\n",
    "dia_y = np.load(dia_target_file) \n",
    "\n",
    "exp_x = np.load(exp_input_file)  \n",
    "exp_y = np.load(exp_target_file)\n",
    "\n",
    "Nt = float(input_file[-6:-4])\n",
    "beta = float(input_file [-11:-8])\n",
    "U = float(input_file [-15:-12])\n",
    "usqrt = np.sqrt(U*beta/Nt)\n",
    "print(\"U  beta  Nt  Usqrt\")\n",
    "print(U,beta,Nt,np.round(usqrt,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data scaling\n",
    "exp_gaus = np.real(exp_y[:10000])\n",
    "exp_HMC = np.real(exp_y[10000:])\n",
    "diag_gaus = dia_y[:10000]\n",
    "diag_HMC = dia_y[10000:]\n",
    "\n",
    "print(\"(min,max) force(exp)\",(np.min(np.real(exp_y)),np.max(np.real(exp_y))),\n",
    "      \"\\n(min,max) force(diag)\",(np.min(np.real(dia_y)),np.max(np.real(dia_y))),\n",
    "      \"\\n(min,max) force Gauss exp\",(np.min(exp_gaus),np.max(exp_gaus)),\n",
    "      \"\\n(min,max) force Gauss diag\",(np.min(diag_gaus),np.max(diag_gaus)),\n",
    "      \"\\n(min,max) force HMC exp\",(np.min(exp_HMC),np.max(exp_HMC)),\n",
    "      \"\\n(min,max) force HMC diag\",(np.min(diag_HMC),np.max(diag_HMC)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outliers and preparing the data\n",
    "def scalingData(a,b,threshold):   \n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    for i in range(b.shape[0]):\n",
    "        if np.max(np.abs(np.real(b[i]))) < threshold :\n",
    "            data_x.append(a[i])\n",
    "            data_y.append(b[i])\n",
    "\n",
    "    data_x = np.array(data_x)\n",
    "    data_y = np.array(data_y)\n",
    "    print(\"inputs=\",data_x.shape,\"targets=\",data_y.shape)\n",
    "    plt.title(\"Targets vs inputs \")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"grad(x)\")\n",
    "    plt.scatter(data_x,data_y)\n",
    "#     plt.savefig(\"plots/scaleddataDIA.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return data_x,data_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaledExp,y_scaledExp = scalingData(exp_x,exp_y,50.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaledDiag,y_scaledDiag = scalingData(dia_x,dia_y,50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(min,max) force(exp)\",(np.min(np.real(y_scaledExp)),np.max(np.real(y_scaledExp))),\n",
    "      \"\\n(min,max) force(diag)\",(np.min(np.real(y_scaledDiag)),np.max(np.real(y_scaledDiag))))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotsData(x,y,c):\n",
    "    mean_phi,abs_phi,mean_grad,abs_grad = [],[],[],[]\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        mean_phi.append(np.mean(x[i]))\n",
    "        abs_phi.append(np.sum(np.abs(x[i])**2))\n",
    "        mean_grad.append(np.mean(y[i]))\n",
    "        abs_grad.append(np.sum(np.abs(y[i])**2))\n",
    "\n",
    "    fig,ax = plt.subplots(2,2,figsize=(15,10))\n",
    "    axx = ax[0][0]\n",
    "    axx.scatter(mean_phi,mean_grad)\n",
    "    axx.set_title(\" Mean Gradient vs  Mean Phi\")\n",
    "    axx.set_xlabel(\"mean[phi]\")\n",
    "    axx.set_ylabel(\"mean[grad(phi)]\")\n",
    "\n",
    "    axx = ax[0][1]\n",
    "    axx.scatter(mean_phi,abs_grad)\n",
    "    axx.set_title(\" Absolute Gradient Square vs  Mean Phi\")\n",
    "    axx.set_xlabel(\"mean[phi]\")\n",
    "    axx.set_ylabel(\"Sum[Abs(grad(phi))^2]\")\n",
    "\n",
    "    axx = ax[1][0]\n",
    "    axx.scatter(abs_phi,mean_grad)\n",
    "    axx.set_title(\" Mean Gradient vs Absolute Phi Square\")\n",
    "    axx.set_xlabel(\"Sum[Abs(phi)^2)]\")\n",
    "    axx.set_ylabel(\"mean[grad(phi)]\")\n",
    "\n",
    "    axx = ax[1][1]\n",
    "    axx.scatter(abs_phi,abs_grad)\n",
    "    axx.set_title(\"Absolute Gradient Squaret vs Absolute Phi Square\")\n",
    "    axx.set_xlabel(\"Sum[Abs(phi)^2)]\")\n",
    "    axx.set_ylabel(\"Sum[Abs(grad(phi))^2]\")\n",
    "\n",
    "    if c == 1:\n",
    "        plt.suptitle('U={}, beta={}, Nt={}\\nsamples(DIA)={}'.format(U,beta,Nt,x.shape[0]),fontsize=20)\n",
    "    else:\n",
    "        plt.suptitle('U={}, beta={}, Nt={}\\nsamples(EXP)={}'.format(U,beta,Nt,x.shape[0]),fontsize=20)\n",
    "#     plt.savefig(\"plots/meanAbsDia.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return mean_phi,abs_phi,mean_grad,abs_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_phiExp,abs_phiExp,mean_gradExp,abs_gradExp = plotsData(np.real(x_scaledExp),np.real(y_scaledExp),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_phiDiag,abs_phiDiag,mean_gradDiag,abs_gradDiag = plotsData(np.real(x_scaledDiag),np.real(y_scaledDiag),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2,figsize=(15,10))\n",
    "\n",
    "axx = ax[0][0]\n",
    "axx.hist(mean_gradExp,bins=50,range=(-5,5))\n",
    "axx.set_title(\"Mean Gradient(EXP)\")\n",
    "\n",
    "\n",
    "axx = ax[0][1]\n",
    "axx.hist(abs_gradExp,bins=50,range=(0,900))\n",
    "axx.set_title(\" Absolute Gradient Square (EXP)\")\n",
    "\n",
    "axx = ax[1][0]\n",
    "axx.hist(mean_gradDiag,bins=50,range=(-5,5))\n",
    "axx.set_title(\"Mean Gradient(DIA)\")\n",
    "\n",
    "\n",
    "axx = ax[1][1]\n",
    "axx.hist(abs_gradDiag,bins=50,range=(0,900))\n",
    "axx.set_title(\" Absolute Gradient Square (DIA)\")\n",
    "\n",
    "# plt.savefig(\"plots/hist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the prior\n",
    "\n",
    "q = np.linspace(-30,30,50)\n",
    "k = 0\n",
    "fig,ax = plt.subplots(8,4,figsize=(8,12))\n",
    "fig.tight_layout()\n",
    "for i in range(8):\n",
    "    for j in range(4):\n",
    "        counts, bins = np.histogram(np.real(exp_y[10000:,k]),bins='auto')\n",
    "        ax[i][j].hist(np.real(exp_y[10000:,k]),bins=bins,density=True)\n",
    "        ax[i][j].plot(q, stats.norm.pdf(q, 0.,1.9)*1.3,'k-')\n",
    "        ax[i][j].set_title(f\"k{k}\")\n",
    "        k = k+1\n",
    "# plt.savefig(\"prior_1.png\")\n",
    "plt.show()\n",
    "\n",
    "# for i in range(3):\n",
    "#     for j in range(3):\n",
    "#         counts, bins = np.histogram(np.real(vy[10000:,k]),bins='auto')\n",
    "#         ax[i][j].hist(np.real(vy[10000:,k]),bins=bins,density=True)\n",
    "#         ax[i][j].plot(q, stats.norm.pdf(q, 0.,1.9)*1.3,'k-')\n",
    "#         ax[i][j].set_title(f\"k{k}\")\n",
    "#         k = k+1\n",
    "#         if k ==32:\n",
    "#             break\n",
    "\n",
    "# # plt.show()\n",
    "# # plt.title(\"Gradient(phi) element-wise EXP\")\n",
    "\n",
    "\n",
    "# # for i in range(32):\n",
    "# #     plt.plot(q, stats.norm.pdf(q,0,1.9)*1.4,'k-')\n",
    "# #     counts, bins = np.histogram(np.real(vy[10000:,i]),bins='auto')\n",
    "# #     plt.hist(np.real(vy[10000:,i]),bins=bins,density=True)\n",
    "   \n",
    "# plt.savefig(\"prior_2.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting all the elements together\n",
    "counts, bins = np.histogram(np.real(exp_y[10000:]),bins='auto')\n",
    "plt.hist(np.real(exp_y[10000:]),bins=bins,density=True)\n",
    "plt.plot(q, stats.norm.pdf(q, 0.,1.9)*1.36,'k-')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    plt.hist(y_scaledDiag[:,i],density=True)\n",
    "\n",
    "plt.title(\"Gradient(phi) element-wise Diag\")\n",
    "# plt.savefig(\"plots/gradHistDia.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.real(x_scaledExp),np.real(y_scaledExp), test_size=0.1,shuffle=True)\n",
    "# X_train = np.real(x_scaledExp)\n",
    "# y_train = np.real(y_scaledExp)\n",
    "# X_test = np.real(x_scaledDiag)\n",
    "# y_test = np.real(y_scaledDiag)\n",
    "print(\"Train data size=\",X_train.shape,\"\\t Test data size=\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = 10000\n",
    "# trial_data = np.zeros((num_samples,xx.shape[1])) # data for testing the NN\n",
    "# for i in range(num_samples): \n",
    "#     trial_data[i,:] = np.random.normal(0,usqrt,xx.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def priorOld(kernel_size, bias_size=0, dtype=None):\n",
    "#     n = kernel_size + bias_size\n",
    "#     prior_model = keras.Sequential(\n",
    "#         [\n",
    "#             tfp.layers.DistributionLambda(\n",
    "#                 lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "#                     loc=tf.zeros(n), scale_diag=tf.ones(n)*usqrt\n",
    "#                 )\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "#     return prior_model\n",
    "\n",
    "# def posteriorOld(kernel_size, bias_size=0, dtype=None):\n",
    "#     n = kernel_size + bias_size\n",
    "#     posterior_model = keras.Sequential(\n",
    "#         [\n",
    "#             tfp.layers.VariableLayer(\n",
    "#                 tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
    "#             ),\n",
    "#             tfp.layers.MultivariateNormalTriL(n),\n",
    "#         ]#activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.0)\n",
    "#     )\n",
    "#     return posterior_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numeric stability the network is parametrized  with $\\rho$ instead of $\\sigma$ directly and transformed to $\\rho$ \n",
    "with the softplus function.\n",
    "The **Posterior** and $\\rho$ are trainable parameters and are intialized by normal gaussian and zero respectively. \n",
    "The **Prior $p(w)$** is defined as :\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma = \\log(1 + e^{\\rho}) = \\mathbf{softplus(\\rho)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{p(w)}= \\pi\\mathbf{\\operatorname{N}(w| 0,\\sigma^{2}_{1})} + (1-\\pi)\\mathbf{\\operatorname{N}(w| 0,\\sigma^{2}_{2})}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mixture_prior_params(sigma_1, sigma_2, pi, return_sigma=False):\n",
    "#     params = K.variable([sigma_1, sigma_2, pi], name='mixture_prior_params')\n",
    "#     sigma = np.sqrt(pi * sigma_1 ** 2 + (1 - pi) * sigma_2 ** 2)\n",
    "#     return params, sigma\n",
    "  \n",
    "# def log_mixture_prior_prob(w):\n",
    "#     comp_1_dist = tf.distributions.Normal(0.0, prior_params[0])\n",
    "#     comp_2_dist = tf.distributions.Normal(0.0, prior_params[1])\n",
    "#     comp_1_weight = prior_params[2]\n",
    "#     return K.log(comp_1_weight * comp_1_dist.prob(w) + (1 - comp_1_weight) * comp_2_dist.prob(w))\n",
    "  \n",
    "# # Mixture prior parameters shared across DenseVariational layer instances\n",
    "# prior_params, prior_sigma = mixture_prior_params(sigma_1=1.0, sigma_2=usqrt, pi=0.0)\n",
    "# tfd = tfp.distributions\n",
    "# def  posterior(kernel_size, bias_size=0, dtype=None):\n",
    "#     n = kernel_size + bias_size\n",
    "#     c = np.log(np.expm1(1.))  # log(1.7182)= 0.54132\n",
    "#     return tf.keras.Sequential([\n",
    "#         tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "#         tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "#             tfd.Normal(loc=t[..., :n],\n",
    "#                        scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
    "#             reinterpreted_batch_ndims=1)),\n",
    "#     ])\n",
    "\n",
    "\n",
    "# # Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\n",
    "# def prior(kernel_size, bias_size=0, dtype=None):\n",
    "#     n = kernel_size + bias_size\n",
    "#     return tf.keras.Sequential([\n",
    "#         tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "#         tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "#             tfd.Normal(loc=t, scale=1.),\n",
    "#             reinterpreted_batch_ndims=1)),\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseVariational(Layer):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 kl_weight,\n",
    "                 activation=None,\n",
    "                 prior_sigma_1=1.707,\n",
    "                 prior_sigma_2=1.0,\n",
    "                 prior_pi=1.1, **kwargs):\n",
    "        self.units = units\n",
    "        self.kl_weight = kl_weight\n",
    "        self.activation = activations.get(activation)\n",
    "        self.prior_sigma_1 = prior_sigma_1\n",
    "        self.prior_sigma_2 = prior_sigma_2\n",
    "        self.prior_pi_1 = prior_pi\n",
    "        self.prior_pi_2 = 0.0\n",
    "        self.init_sigma = 1.8\n",
    "#         self.init_sigma = np.sqrt(self.prior_pi_1 * self.prior_sigma_1 ** 2 +\n",
    "#                                   self.prior_pi_2 * self.prior_sigma_2 ** 2)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel_mu = self.add_weight(name='kernel_mu',\n",
    "                                         shape=(input_shape[1], self.units),\n",
    "                                         initializer=tf.keras.initializers.RandomNormal(stddev=self.init_sigma),\n",
    "                                         trainable=True)\n",
    "        self.bias_mu = self.add_weight(name='bias_mu',\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer=tf.keras.initializers.RandomNormal(stddev=self.init_sigma),\n",
    "                                       trainable=True)\n",
    "        self.kernel_rho = self.add_weight(name='kernel_rho',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                          initializer=initializers.constant(0.0),\n",
    "                                          trainable=True)\n",
    "        self.bias_rho = self.add_weight(name='bias_rho',\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=initializers.constant(0.0),\n",
    "                                        trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):  # backward pass\n",
    "        kernel_sigma = tf.math.softplus(self.kernel_rho)\n",
    "#         kernel_sigma = 0.0\n",
    "        kernel = self.kernel_mu + kernel_sigma * tf.random.normal(self.kernel_mu.shape) # mu + sigma*epsilon\n",
    "\n",
    "        bias_sigma = tf.math.softplus(self.bias_rho)\n",
    "        bias = self.bias_mu + bias_sigma * tf.random.normal(self.bias_mu.shape) # mu + sigma*epsilon\n",
    "\n",
    "        self.add_loss(self.kl_loss(kernel, self.kernel_mu, kernel_sigma) +\n",
    "                      self.kl_loss(bias, self.bias_mu, bias_sigma))\n",
    "\n",
    "        return self.activation(K.dot(inputs, kernel) + bias)\n",
    "\n",
    "    def kl_loss(self, w, mu, sigma):\n",
    "        variational_dist = tfp.distributions.Normal(mu, sigma)\n",
    "        return self.kl_weight * K.sum(variational_dist.log_prob(w) - self.log_prior_prob(w))\n",
    "\n",
    "    def log_prior_prob(self, w):\n",
    "        comp_1_dist = tfp.distributions.Normal(0.0, self.prior_sigma_1)\n",
    "        comp_2_dist = tfp.distributions.Normal(0.0, self.prior_sigma_2)\n",
    "        return K.log(self.prior_pi_1 * comp_1_dist.prob(w) +\n",
    "                     self.prior_pi_2 * comp_2_dist.prob(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "batch_size = 80\n",
    "kl_loss_weight = 1.0 / (X_train.shape[0]/batch_size)\n",
    "\n",
    "print(\"kl_loss_weight=\",kl_loss_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_params = {\n",
    "    'prior_sigma_1': 1.807, \n",
    "    'prior_sigma_2': 1.0, \n",
    "    'prior_pi': 1.1\n",
    "}\n",
    "\n",
    "x_in = Input(shape=(32,))\n",
    "x = DenseVariational(32, kl_loss_weight, **prior_params, activation='relu')(x_in)\n",
    "# x = DenseVariational(32, kl_loss_weight, **prior_params, activation='relu')(x)\n",
    "# x = DenseVariational(32, kl_loss_weight, **prior_params)(x)\n",
    "x = layers.Dense(units=32,)(x)\n",
    "model = Model(x_in, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build Model\n",
    "# # tf.compat.v1.disable_eager_execution()\n",
    "# model_2 = tf.keras.Sequential([\n",
    "#     tf.keras.layers.InputLayer(input_shape=(32,)),\n",
    "#     tfp.layers.DenseVariational(units=32,\n",
    "#                                 make_posterior_fn=posterior,\n",
    "#                                 make_prior_fn=prior,\n",
    "#                                 kl_weight=kl_loss_weight,\n",
    "#                                 activation='relu'),\n",
    "# #     tfp.layers.DenseVariational(units=32,\n",
    "# #                                 make_posterior_fn=posterior,\n",
    "# #                                 make_prior_fn=prior,\n",
    "# #                                 kl_weight=kl_loss_weight,\n",
    "# #                                 activation='relu'),\n",
    "#     layers.Dense(units=32,)\n",
    "# #      tfp.layers.DenseVariational(units=32,\n",
    "# #                                 make_posterior_fn=posterior,\n",
    "# #                                 make_prior_fn=prior,\n",
    "# #                                 kl_weight=kl_loss_weight)\n",
    "# ])\n",
    "# model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, loss, X_train,y_train,X_test,y_test):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "        run_eagerly=False\n",
    "    )\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(X_train,y_train, epochs=num_epochs,batch_size=batch_size, \n",
    "                        validation_data=(X_test,y_test),verbose=0,\n",
    "                        callbacks=[TqdmCallback(verbose=1)])\n",
    " #                         callbacks=[TQDMNotebookCallback(leave_inner=True,leave_outer=True)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Model training finished.\")\n",
    "    # \"Accuracy\"\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    # \"Loss\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.MeanAbsoluteError()\n",
    "# loss = keras.losses.MeanSquaredError()\n",
    "# loss = tf.keras.losses.LogCosh()\n",
    "\n",
    "\n",
    "# def neg_log_likelihood(y_obs, y_pred, sigma=1.9):\n",
    "#     dist = tfp.distributions.Normal(loc=y_pred, scale=sigma)\n",
    "#     return K.sum(-dist.log_prob(y_obs))\n",
    "\n",
    "# loss = neg_log_likelihood\n",
    "\n",
    "print(\"Number of batches in:\\ntrain_dataset=\",int(X_train.shape[0]/batch_size),\n",
    "      \"\\ntest dataset=\",int(X_test.shape[0]/batch_size))\n",
    "\n",
    "run_experiment(model, loss,X_train,y_train,X_test,y_test)\n",
    "loss_1, accuracy_1 = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f'% (accuracy_1*100), 'Loss: %.2f' % (loss_1))\n",
    "\n",
    "# run_experiment(model_2, loss,X_train,y_train,X_test,y_test)\n",
    "# loss_2, accuracy_2 = model_2.evaluate(X_test, y_test)\n",
    "# print('Accuracy: %.2f'% (accuracy_2*100), 'Loss: %.2f' % (loss_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data\n",
    "#ex =[1.8506733132414595, -0.5413392919167934, -0.08290738562338233, 0.21020703115540967, 0.45736532200026686, 0.4144505763882332, 0.7974591877817361, -1.0755342176198304, -1.2923531024863615, 0.7928643052728441, 1.8700508128542486, 0.9516103164912759, -0.4170465613156157, -0.5803716218887971, -1.4581616083955689, -0.40999934680233296, -1.6091129317275288, -0.4340613520847116, 0.11684358400103442, -0.1907166017062774, 1.2559540251043124, 0.15898709633936114, 0.21429222182377086, -0.409223823105235, 1.572301166661345, 0.4515499421024908, 0.4609354654439869, -0.15711651718793024, -0.056095371688984985, -0.8742907599873543, -0.0343957073741717, 0.4250477713080214]\n",
    "# ex = np.array(ex).reshape(1,32)\n",
    "# g = [49.86549443077032, -32.829898739627914, 31.066976142463147, -16.39544920290602, 23.644040843599672, -7.483481398329604, 19.671315362354513, -5.810537773537636, 15.51302201582224, -2.0950719617562044, 19.184476694104273, 0.8757732130798428, 14.160922938674743, -1.738831656590501, 13.75108490801291, -3.0704791699156475, 15.426648483802477, -5.096069402933892, 19.006163317993384, -4.736981704910806, 25.584616583763687, -8.337806692383278, 31.611048236397345, -17.583983790467204, 44.126115313888214, -25.66148544786747, 43.56402785659327, -28.53946231158809, 42.269855498042155, -29.71370011290176, 36.99478211467913, -21.796550338318383]\n",
    "# g = np.array(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = np.array(np.real(exp_x[0])).reshape(1,32)\n",
    "example_target =np.real(exp_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_1 = []\n",
    "predicted_2 = []\n",
    "iterations = 1\n",
    "for _ in range(iterations):\n",
    "    predicted_1.append(model(example_input).numpy())\n",
    "#     predicted_2.append(model_2(example_input).numpy())\n",
    "    \n",
    "pre = np.array(predicted_1)\n",
    "# print(pre.shape)\n",
    "\n",
    "predicted_1 = np.concatenate(np.array(predicted_1), axis=0)\n",
    "# predicted_2 = np.concatenate(np.array(predicted_2), axis=0)\n",
    "print(predicted_1.shape)\n",
    "\n",
    "prediction1_mean = np.mean(predicted_1, axis=0).tolist()\n",
    "# prediction2_mean = np.mean(predicted_2, axis=0).tolist()\n",
    "# print(np.array(prediction1_mean).shape)\n",
    "\n",
    "# prediction1_min = np.min(predicted_1, axis=1).tolist()\n",
    "# prediction1_max = np.max(predicted_1, axis=1).tolist()\n",
    "# prediction1_range = (np.max(predicted_1, axis=1) - np.min(predicted_1, axis=1)).tolist()\n",
    "\n",
    "# prediction2_min = np.min(predicted_2, axis=1).tolist()\n",
    "# prediction2_max = np.max(predicted_2, axis=1).tolist()\n",
    "# prediction2_range = (np.max(predicted_2, axis=1) - np.min(predicted_2, axis=1)).tolist()\n",
    "\n",
    "for idx in range(1):\n",
    "#     print(\n",
    "#         f\"Predictions mean: {round(prediction1_mean[idx], 2)}, \"\n",
    "# #         f\"min: {round(prediction1_min[idx], 2)}, \"\n",
    "#         f\"max: {round(prediction1_max[idx], 2)}, \"\n",
    "#         f\"range: {round(prediction1_range[idx], 2)} - \"\n",
    "    \n",
    "    print(\n",
    "        f\"Predictions mean: {np.array(prediction1_mean)}, \"\n",
    "#         f\"min: {round(prediction2_min[idx], 2)}, \"\n",
    "#         f\"max: {round(prediction2_max[idx], 2)}, \"\n",
    "#         f\"range: {round(prediction2_range[idx], 2)} - \"\n",
    "    )\n",
    "    print(\"actual target=\",example_target)\n",
    "    print(\"difference in prediction and actual=\",np.array(prediction1_mean)-np.array(example_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
